{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7c0b40",
   "metadata": {},
   "source": [
    "# TD2 part 2: Named entity recognition\n",
    "\n",
    "Dans ce TD, nous allons prendre un datasets où les noms de personnes sont taggés.<br>\n",
    "Nous allons transformer ces données en tenseurs X, y et attention_mask.<br>\n",
    "Nous allons créer un modèle RNN pour prédire si un mot est un nom de personne.<br>\n",
    "Nous allons ensuite créer la loop avec l'optimizer pour apprendre le modèle.<br>\n",
    "Du modèle appris (prédisant sur les tokens), nous allons postprocess les prédictions pour avoir les prédictions sur les noms.\n",
    "\n",
    "Un fois que la loop est créée et que le modèle apprend, nous allons changer la structure du modèle:\n",
    "- Changer learning rate. Comment se comporte le modèle\n",
    "- Ajouter des couches denses, ReLU, dropout, normalization\n",
    "- Changer le nombre de layers du RNN, LSTM.\n",
    "\n",
    "Lorsqu'on a un bon modèle de prédiction pour les noms de personnes, nous allons l'appliquer à notre projet fil rouge.\n",
    "Utilisez-le tel que. Quelle accuracy ?\n",
    "Ré-entrainez la (les) dernière(s) couche(s) du modèle sur notre jeu de données. A-t-il gagné en accuracy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86402ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divino/miniconda3/envs/school-nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f552fb",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Télécharger le dataset MultiNERD FR [ici](https://github.com/Babelscape/multinerd)<br>\n",
    "Mettez les données dans le dossier data/raw du projet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157bc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_multinerd_person_words(filename=\"../data/raw/train_fr.tsv\"):\n",
    "    with open(filename) as f:\n",
    "        tagged_words = [line.strip().split(\"\\t\") for line in f]\n",
    "        \n",
    "        # Joining words until we meet a dot\n",
    "        # Word's label is 1 if 'PER' is in its tag\n",
    "        sentences = []\n",
    "        sentence_labels = []\n",
    "    \n",
    "        this_word = []\n",
    "        this_labels = []\n",
    "        for tagged_word in tagged_words:\n",
    "            if len(tagged_word) < 3:\n",
    "                # not a tagged word\n",
    "                continue\n",
    "            word = tagged_word[1]\n",
    "            tag = tagged_word[2]\n",
    "        \n",
    "            if word == '.':\n",
    "                sentences.append(\" \".join(this_word))\n",
    "                sentence_labels.append(this_labels)\n",
    "            \n",
    "                this_word = []\n",
    "                this_labels = []\n",
    "            else:\n",
    "                this_word.append(word)\n",
    "                this_labels.append(1 * tag.endswith(\"PER\"))\n",
    "\n",
    "    return sentences, sentence_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcba104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = extract_multinerd_person_words(\"../data/raw/multinerd_train.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b09cc",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "En utilisant le tokenizer d'HuggingFace \"camembert-base\":\n",
    "- Transformer les phrases en tokens\n",
    "- Obtenez des vecteur y qui ont le même nombre d'entrées qu'il y a de tokens dans la phrase\n",
    "- Ayez un tenseur \"attention_mask\" pour savoir sur quels tokens on cherche à predire le label\n",
    "- Transformez les tokens en token_ids (avec le tokenizer)\n",
    "Avec tout cela, vous pouvez former vos tenseurs X, Y et attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8ff1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a3802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokens_and_labels_and_attention_mask(tokenizer, sentence, labels):\n",
    "    words = sentence.split()\n",
    "    tokens = []\n",
    "    tokens_label = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for word, label in zip(words, labels):\n",
    "        this_tokens = tokenizer.tokenize(word)\n",
    "        tokens += this_tokens\n",
    "        \n",
    "        this_labels = [0] * len(this_tokens)\n",
    "        this_labels[0] = label        \n",
    "        tokens_label += this_labels\n",
    "        \n",
    "        this_attention_mask = [1] + [0] * (len(this_tokens) - 1)\n",
    "        attention_mask += this_attention_mask\n",
    "        \n",
    "    return tokens, tokens_label, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066d1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, label, padding_masks = build_tokens_and_labels_and_attention_mask(tokenizer, sentences[0], labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb94a39",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Contruisez un modèle RNN comme dans la partie 1. Pour l'instant, il prendra comme arguments:\n",
    "- Vocab size: le nombre de différents tokens du tokenizer (52 000 pour camembert-base)\n",
    "- Embedding dim: la dimension de l'embedding des tokens (par défaut 50)\n",
    "- hidden_dim: la dimension de l'état récurrent de votre RNN (par défaut 20)\n",
    "- tagset_size: la nombre de classes possibles pour les prédictions (ici 2)\n",
    "\n",
    "Dans le forward, votre modèle enchaînera les couches suivantes:\n",
    "- Un embedding\n",
    "- Un RNN\n",
    "- Un ReLU\n",
    "- Une couche linéaire\n",
    "- Un softmax pour que la somme des prédictions pour une entrée soit égale à 1 (la prédiction pour un élément et sa probabilité d'être dans chaque classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e661ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0235d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=20, tagset_size=2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Initialisation des couches\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passage dans le modèle\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04004a",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Je fournis ici une fonction prenant un modèle, des tenseurs X, y, attention_mask.\n",
    "Pour chaque batch:\n",
    "- La loop utilise le modèle pour prédire sur x_batch\n",
    "- Avec attention_mask, elle identifie sur quels tokens les prédictions compte\n",
    "- Elle regarde la cross entropy entre y\\[attention_ids\\] et yhat\\[attention_ids\\]\n",
    "- Elle output un dictionnaire avec le model et la loss au fur et à mesure des itérations\n",
    "\n",
    "Entraînez le modèle avec vos données. <br>\n",
    "Plottez la loss history.<br>\n",
    "Itérez sur le modèle pour l'améliorer:\n",
    "- Changer learning rate. Comment se comporte le modèle\n",
    "- Ajouter des couches denses, ReLU, dropout, normalization\n",
    "- Changer le nombre de layers du RNN, LSTM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58b9833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def train_model(model, X, y, attention_masks, n_epochs=100, lr=0.05, batch_size=128):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    loss_history = []\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, y, attention_masks)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"epoch:\", epoch)\n",
    "        for i, (x_batch, y_batch, mask) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ids = mask.reshape(-1)\n",
    "            yhat = model(x_batch).reshape((-1, 2))[ids]\n",
    "            this_y = y_batch.reshape(-1)[ids]\n",
    "\n",
    "            loss = loss_function(yhat, this_y)\n",
    "            loss.backward()\n",
    "\n",
    "            loss_history.append(loss.clone().detach())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Got loss at {epoch}\", np.mean(loss_history[-10:]))\n",
    "\n",
    "    return {\"model\": model, \"loss_history\": loss_history}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cf63f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 52000  # Pour camembert-base\n",
    "model = RNNModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f99489db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8630/409453289.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  X = torch.tensor([np.pad(token_ids, (0, max_length - len(token_ids)), mode='constant') for token_ids in X])\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour construire les tenseurs à partir des données\n",
    "def build_tensors(tokenizer, sentences, labels):\n",
    "    X = []\n",
    "    y = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        tokens, tokens_label, attention_mask = build_tokens_and_labels_and_attention_mask(tokenizer, sentence, label)\n",
    "\n",
    "        # Convertissez les tokens en ids\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        X.append(token_ids)\n",
    "        y.append(tokens_label)\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    # Padding pour garantir que toutes les séquences aient la même longueur\n",
    "    X = torch.tensor([np.pad(token_ids, (0, max_length - len(token_ids)), mode='constant') for token_ids in X])\n",
    "    y = torch.tensor([np.pad(label, (0, max_length - len(label)), mode='constant') for label in y])\n",
    "    attention_masks = torch.tensor([np.pad(mask, (0, max_length - len(mask)), mode='constant') for mask in attention_masks])\n",
    "\n",
    "    return X, y, attention_masks\n",
    "\n",
    "# Création des tenseurs\n",
    "max_length = max([len(tokenizer.encode(sentence)) for sentence in sentences])  # Longueur maximale des séquences\n",
    "X, y, attention_masks = build_tensors(tokenizer, sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50cbb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([np.pad(token_ids, (0, max_length - len(token_ids)), mode='constant') for token_ids in X], dtype=torch.long)\n",
    "y = torch.tensor([np.pad(label, (0, max_length - len(label)), mode='constant') for label in y], dtype=torch.long)\n",
    "attention_masks = torch.tensor([np.pad(mask, (0, max_length - len(mask)), mode='constant') for mask in attention_masks], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbd81706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    }
   ],
   "source": [
    "results = train_model(model, X, y, attention_masks, n_epochs=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b623256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(results['loss_history'])\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a6933",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "Créer une fonction prenant les prédictions du modèle (au niveau token) et sort les prédictions au niveau mot.<br>\n",
    "Par exemple, admettons que, pour un mot, la prédiction du 1er token est la seule qu'on considère.<br>\n",
    "si la phrase est \"Bonjour John\", avec les tokens \\[\"bon\", \"jour\", \"Jo\", \"hn\"\\] avec les predictions \\[0.12, 0.65, 0.88, 0.45\\]<br>\n",
    "Je veux récupérer les prédictions \"bonjour\": 0.12, \"John\": 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0316b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions(tokens, predictions, attention_masks):\n",
    "    word_predictions = {}\n",
    "    current_word = \"\"\n",
    "    current_prediction = 0.0\n",
    "\n",
    "    for token, prediction, mask in zip(tokens, predictions, attention_masks):\n",
    "        if mask == 1:  # Début d'un nouveau mot\n",
    "            if current_word:  # Si un mot précédent existe, l'ajouter au dictionnaire\n",
    "                word_predictions[current_word] = current_prediction\n",
    "            \n",
    "            current_word = token  # Commencer un nouveau mot\n",
    "            current_prediction = prediction\n",
    "        else:\n",
    "            current_word += token  # Continuer à construire le mot actuel\n",
    "\n",
    "    # Ajouter le dernier mot s'il existe\n",
    "    if current_word:\n",
    "        word_predictions[current_word] = current_prediction\n",
    "\n",
    "    return word_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8562e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    # Tokenisation\n",
    "    tokens, tokens_label, attention_mask = build_tokens_and_labels_and_attention_mask(tokenizer, sentence, labels)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_ids_tensor = torch.tensor([token_ids])\n",
    "\n",
    "    # Obtenir les prédictions du modèle\n",
    "    with torch.no_grad():\n",
    "        model_output = model(token_ids_tensor)\n",
    "    predictions = torch.softmax(model_output, dim=2)\n",
    "    predictions = predictions[0, :, 1]\n",
    "\n",
    "    # Post-traitement pour obtenir les prédictions au niveau des mots\n",
    "    word_predictions = postprocess_predictions(tokens, predictions.tolist(), attention_mask)\n",
    "\n",
    "    print(f\"Prédictions pour '{sentence}': {word_predictions}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
